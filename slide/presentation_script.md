# 卒業論文発表原稿

**発表時間**: 12分
**質疑応答**: 3分

---

## 1. タイトルスライド (0:00 - 0:30)

ただいまご紹介にあずかりました、東京理科大学 創域理工学部 情報計算科学科の三笠です。
「一点物売買プラットフォームの動作検証を目的とした社会的シナリオに則ったデータ生成エージェントの開発」という題目で発表させていただきます。
よろしくお願いいたします。

## 2. 研究背景: クリエイターエコノミーと「文脈」 (0:30 - 1:30)

まず、研究の背景についてお話しします。
近年、クリエイターエコノミーの拡大に伴い、個人が自身の創作物を発信・販売する市場が急速に成長しています。特に、フィギュアや工芸品といった物理的な一点物作品の流通も活発化しています。

しかし、既存のプラットフォームには課題があります。
多くの場合、作品はスペックや価格のみで比較される「商品」として扱われがちです。
一点物作品において本来価値の源泉となる、「なぜ作ったのか」「どのようなコミュニティで生まれたのか」といった「文脈」や「物語性（ナラティブ）」が、システム上で十分に保存・表現されていないのが現状です。
また、共同制作やイベント運営など、クリエイター間の複雑な社会的関係性や収益分配をサポートする機能も不足しています。

そこで、作品の「文脈」や「社会的関係性」を構造化して保存できる、新しいCtoCプラットフォームが必要とされています。

## 3. 開発上の課題: 「テストデータがない」 (1:30 - 2:30)

しかし、こうした「文脈」を重視するプラットフォームを開発しようとすると、大きな壁にぶつかります。それは「テストデータが存在しない」という問題です。

システムのUIや検索機能を検証するためには、「文脈を持ったデータ」が不可欠です。
しかし、Faker.jsなどの従来のランダム生成手法では、「部外者がプロジェクトの作品を管理している」といった論理的な矛盾が頻発してしまいます。また、「Lorem Ipsum」のような無意味なテキストでは、文脈検索の精度を検証することができません。

さらに、これは新規開発であるため、学習元となる過去のデータも存在しません。かといって、複雑なリレーションを持つデータを手動で大量に作成するのはコスト的に不可能です。

つまり、整合性とリアリティのあるデータを、自動で大量に生成する手法が求められています。

## 4. 研究目的 (2:30 - 3:00)

そこで本研究では、以下の2点を目的としました。

第一に、一点物作品のナラティブを保存可能な、複雑なリレーションを持つCtoCプラットフォームを実際に開発することです。これは提案手法の検証環境としても機能します。

そして第二に、これが本研究の主眼ですが、LLM（大規模言語モデル）を用いて、データを単なるレコードとしてではなく、「社会的シナリオ（物語）」として生成するエージェントを開発し、その有効性を検証することです。

## 5. 提案手法: 社会的シナリオ生成エージェント (3:00 - 3:45)

提案手法について説明します。
本研究では、データベースの整合性を保つために、データを「点」ではなく「物語」として生成するアプローチをとりました。
具体的には、「ユーザー群」「プロジェクト」「作品群」を、「ひとまとまりのシナリオ」として生成します。

生成プロセスには、映画制作のような役割分担モデルを採用しました。
全体を統括する「Director（監督）」、物語を記述する「Scenarist（脚本）」、そしてそれをデータベースの形式に落とし込む「Designer（設計）」という複数の専門家エージェントが連携してデータを構築します。

## 6. システムアーキテクチャ (LangGraph) (3:45 - 4:30)

システムアーキテクチャの概要図です。
本システムは、LangGraphを用いたステートマシンとして実装されています。

まず、Directorが「次はプラスチック盆栽学会のようなテーマでいこう」といったコンセプトを立案します。
それを受けて、Scenaristが具体的なストーリーや登場人物の背景を記述します。
次に、Designerがそれをシステム固有のJSON形式に変換し、Saverがデータベースへの保存と、ベクトルデータベースへの記憶を行います。
そして、この記憶を再びDirectorが参照するというループ構造になっています。

## 7. 技術的核: RAGによる「記憶」と「重複回避」 (4:30 - 5:15)

本システムの技術的な核となるのが、RAG（検索拡張生成）を用いた「記憶」と「重複回避」の仕組みです。

エージェントは、自分が過去に生成したシナリオを全て記憶しています。
新しいシナリオを生成する際、まず過去の類似したシナリオを検索します。
そして、プロンプトにおいて「これら過去の事例とは異なる、新しい切り口で生成せよ」という制約を課します。

これにより、LLM特有の「似たような回答を繰り返す」というモード崩壊を防ぎ、常に新しいトピックを開拓し続けることを可能にしました。

## 7.1. 実験環境: 開発したプラットフォーム (5:15 - 6:00)

検証環境として開発したプラットフォームについて説明します。
技術スタックには、Next.js, NestJS, PostgreSQLを採用し、決済基盤にはStripe Connectを使用しています。

機能としては、招待制サークルや公募制コンテストを実現する「Project機能」、オークションや抽選販売、複雑な収益分配を行う「高度な決済機能」、そしてユーザー、作品、プロジェクトを横断して検索できる「統合検索機能」を実装しました。

## 7.2. 実験環境: 生成対象となるデータモデル (6:00 - 6:45)

データ生成エージェントが生成する対象のデータモデルです。

中心となるのが「User」で、クリエイターとしてのプロフィールを持ちます。
次に「Project」です。これは企画やサークルといった「文脈」を表すエンティティで、Userがオーナーとなったり、メンバーとして参加したりします。
そして「Item」です。これが作品や成果物を表し、詳細なナラティブを持ち、特定のProjectに紐づきます。

エージェントは、これら3つのエンティティと、その間の複雑なリレーションを、矛盾なくひとまとまりのセットとして生成します。

## 8. 評価実験の設定 (6:45 - 7:15)

提案手法の有効性を検証するため、1000件のシナリオ連続生成実験を行いました。

比較条件として、Gemini 3 Pro および GPT-5.2 の2つのモデルを使用し、それぞれについて記憶機構（RAG）を「ONにした場合」と「OFFにした場合」で比較を行いました。
仮説として、RAGをONにすることで、ネタ被りが減り、データの多様性と品質が向上すると考えられます。

## 9. 結果1: 多様性の向上 (語彙数推移) (7:15 - 8:15)

ここから結果について述べます。
まず、生成されたテキストに含まれる「ユニーク単語数」の増加推移です。

グラフの暖色系の線がRAG OFFの場合です。後半にかけて傾きが緩やかになり、語彙が飽和している、つまり同じような言葉や設定を使い回していることがわかります。
一方、寒色系の線がRAG ONの場合です。こちらは直線的な増加を維持しており、OFFの場合と比較して約30%から40%も多くの語彙を使用しています。
これは、エージェントが常に新しいトピックを開拓し続けていることを示しています。

## 10. 結果2: 重複の抑制 (ベクトル類似度) (8:15 - 9:00)

次に、生成されたデータ間の類似度の分布です。

右側のグラフ、RAG OFFの場合（橙色）は、類似度が高い部分に分布が偏っており、自己複製的な生成が頻発していることがわかります。
一方、RAG ONの場合（青色）は、全体的に分布が左側、つまり類似度が低い方へシフトしており、似たようなシナリオの生成を効果的に抑制できていることが確認できました。

## 11. 結果3: 品質の向上 (定性評価) (9:00 - 10:15)

最後に、定性的な評価結果です。
LLM Judgeによるスコア評価では、RAG ONの場合に「人間らしさ」の項目で高い評価が得られました。

特筆すべきは、内容の質的な変化です。
重複を回避しようとする圧力がかかった結果、エージェントは「ありきたりなアイデア」を出せなくなりました。その結果、深い探索が行われ、「プラスチック盆栽学会」や「不快UIの物理化」といった、極めてニッチでマニアックな設定が出現するようになりました。
AI特有の「無難さ」が消え、こだわりや偏愛を感じさせる「人間味のある」データが生成されたことは、本研究の重要な発見です。

## 12. まとめ (10:15 - 11:00)

まとめです。
本研究では、複雑なリレーションを持つWebシステムのテストデータ生成において、LLMエージェントとRAGを組み合わせた手法を提案し、その有効性を実証しました。
1000件規模の生成実験の結果、記憶機構が単にネタ被りを防ぐだけでなく、データの具体性やリアリティを向上させることにも寄与することがわかりました。

これにより、学習データのない開発初期段階においても、実運用に近い「文脈のあるデータ」を用意することが可能になり、検索システムなどの検証精度を大幅に向上させることができます。

## 13. 今後の展望 (11:00 - 11:45)

最後に今後の展望です。
まずは、生成されたこの「文脈データ」を活用し、高度な検索システムやレコメンデーションエンジンを実際に開発・評価することで、本手法の実践的な有用性を確認したいと考えています。
また、今回はCtoCプラットフォームに特化しましたが、このアーキテクチャをSNSやゲームのNPC生成など、他のドメインにも適用できるよう一般化を進めていきます。
さらに、数万件規模のデータ生成に対応するため、処理の高速化にも取り組む予定です。

以上で発表を終わります。ご清聴ありがとうございました。
