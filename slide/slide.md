---
marp: true
theme: ./theme
paginate: true
header: "情報検索特論 課題発表"
footer: "2025/12/11"
style: |
  section {
    font-family: 'Hiragino Kaku Gothic ProN', 'Meiryo', sans-serif;
    font-size: 30px;
    padding: 50px;
  }
  h1 {
    color: #00509d;
  }
  h2 {
    color: #00509d;
    border-bottom: 2px solid #00509d;
  }
  code {
    background: #f0f0f0;
    color: #d63384;
  }
---

# RDB を用いた Web システムにおける

# pg_bigm を用いた日本語全文検索の実装

### 〜 JSON データへの式インデックス適用 〜

学籍番号: 6322087
氏名: 三笠 悠太郎

<!-- では、「RDB を用いた Web システムにおける pg_bigm を用いた日本語全文検索の実装」と、JSONデータへのインデックス適用手法について発表させていただきます。 -->

---

## 1. 背景：RDB における検索の課題

### 一般的な Web システムの現状

- データの保存先として **RDB (PostgreSQL 等)** が標準的
- 単純な検索機能として SQL の `LIKE '%keyword%'` (部分一致)

### 課題：計算量の爆発

- **シーケンシャルスキャン (全件走査)** が発生
- データ量 $N$ に対し、計算量が $O(N)$ で増加
- サービス規模拡大に伴い、応答速度が致命的に低下

➡️ **$O(1)$ に近づける「転置インデックス」の導入が必須**

<!-- まず背景です。一般的なWebシステムにおいて、データの保存先にはPostgreSQLなどのRDBが標準的に使われます。
しかし、検索機能の実装において、安易にSQLの LIKE 演算子による部分一致検索を採用してしまうと、計算量の問題に直面します。
これはデータベース内部で「シーケンシャルスキャン（全件走査）」が発生するためで、データ量 N に対して計算量が O(N) (オーダーエヌ) で増加します。これではサービスの規模拡大に伴い、応答速度が致命的に低下してしまいます。
これを解決し、検索速度を O(1) (オーダーいち）に近づけるためには、「転置インデックス」の導入が不可欠です。 -->

---

## 2. 理論：日本語検索の難しさ

### 英語 vs 日本語

- **英語:** 単語がスペースで区切られている (分かち書き)
- **日本語:** 単語の区切りが曖昧

### アプローチの比較

| 手法           | 仕組み                        | メリット                     | デメリット                                               |
| :------------- | :---------------------------- | :--------------------------- | :------------------------------------------------------- |
| **形態素解析** | 辞書ベースで<br>単語分解      | 意味単位で<br>正確           | **辞書にない新語・型番**<br>が検索できない (Recall 低下) |
| **N-gram**     | $N$文字ごとに<br>機械的に分割 | **検索漏れなし**<br>辞書不要 | インデックスサイズが大<br>ノイズが増える可能性           |

<!-- しかし、日本語の検索システム構築には難しさがあります。英語のようにスペースで単語が区切られていないためです。 アプローチとしては主に「形態素解析」と「N-gram」があります。 形態素解析は意味単位で正確ですが、辞書にない「新語」や「型番」が検索できないという、Recall（再現率）の低下が課題となります。 一方、N-gramは機械的に文字を分割するためインデックスサイズは大きくなりますが、検索漏れがないという強力なメリットがあります。 -->

---

## 3. 理論：N-gram の仕組み

テキストを「$N$文字ごとの連続した断片（トークン）」として切り出し、インデックスを作成する。

### 例：2-gram (Bigram) の場合

入力文字列：**「東京都」**

<br>

- 「東京」というトークンを持つ ID リスト $\rightarrow$ `[1, 5, 8]`
- 「京都」というトークンを持つ ID リスト $\rightarrow$ `[2, 5, 9]`

➡️ **「未知語（スラング・型番）」に強い N-gram を実装してみた**

<!-- N-gramの仕組みを簡単に確認します。 例えば「東京都」という文字列を2-gram（Bigram）で処理する場合、「東京」「京都」という2文字ごとの断片に切り出し、これらをトークンとしてインデックスを作成します。 これにより、辞書に載っていないようなスラングや製品の型番であっても、文字の並びさえ合致していれば確実に検索できるシステムとなります。 -->

---

## 4. pg_bigm (2-gram) の採用

PostgreSQL 標準の `pg_trgm` (Trigram: 3 文字) ではなく、拡張機能 `pg_bigm` を採用。

### Trigram (3-gram) の弱点

- **3 文字未満の単語**（例：「京都」「戦車」「AI」）の検索に弱い
- パディング処理等のオーバーヘッドや精度低下のリスク

### Bigram (pg_bigm) の優位性

- 日本語は「漢字 2 文字」で意味を成す単語が多い
- **1 文字〜2 文字の短いキーワードでも検索漏れ (False Negative) がない**

<!-- 今回の実装では、PostgreSQL標準の pg_trgm（Trigram）ではなく、拡張機能の pg_bigm を採用しました。 Trigramは3文字単位であるため、「京都」や「戦車」といった2文字の単語の検索に弱く、精度の低下や検索漏れのリスクがあります。 対してBigramを用いる pg_bigm （ぴーじーばいぐらむ）は、漢字2文字で意味を成すことの多い日本語と相性が良く、1文字から2文字の短いキーワードでも検索漏れ（False Negative）が発生しないという優位性があります。 -->

---

## 5. 実装上の課題

### リッチテキストエディタの普及

- 説明文はマークダウンテキストとして出力される
- 本文データが単純な `TEXT` 型ではなく **`JSON` (構造化データ)** として保存された

### データベースの壁

- 通常、全文検索インデックスは `TEXT` 型にしか貼れない

**課題：この JSON の深い階層にある単語をどう検索するか？**
**課題：String 型として保存されていないテキストデータにどうやってインデックスを張るのか？**

<!-- 次に、実装上の課題です。 近年普及しているリッチテキストエディタ（今回はBlockNoteを使用）のデータは、単純なテキストではなく、Markdownなどの構造を持った「JSON形式」で保存されることが一般的です。 しかし、通常RDBの全文検索インデックスは TEXT 型にしか張ることができません。 JSONの深い階層にある単語をどう検索するか、つまり「String型として保存されていないデータにどうインデックスを張るか」が課題となりました。 -->

---

## 6. 回避策：式インデックス (Functional Index)

アプリケーション側でテキスト加工を行わず、DB 機能だけで解決

### 解決策

JSON データを DB 側で強制的に **テキストキャスト** し、その結果にインデックスを作成。
つまり JSON を平文として解釈しただけ

```sql
-- 実際のマイグレーションSQL
CREATE INDEX "item_search_idx" ON "Item"
USING gin ((description::text) gin_bigm_ops);
```

1. description::text で JSON 全体を文字列化

2. pg_bigm で 2-gram トークン化してインデックス生成
<!-- この解決策として、「式インデックス（Functional Index）」を採用しました。 アプリケーション側でJSONをパースして検索用カラムを作るのではなく、DBの機能だけで解決します。 具体的にはスライドのSQLのように description::text という記述を用います。 これはJSONデータ全体を強制的にテキストキャスト、つまり平文の文字列として解釈し、その結果に対して pg_bigm のインデックスを作成する手法です。これにより、構造化データの中身も含めた全文検索を実現しました。 -->

---

## 7. システム構成

- App: NestJS (Node.js) + Prisma ORM

- DB: PostgreSQL 15 + pg_bigm

### 工夫点：Raw SQL の活用

ORM (Prisma) は JSON への全文検索インデックス作成を標準サポートしていないため、Raw SQL (生 SQL) を用いてマイグレーション管理とクエリ実行を実装。

<!-- システム構成は、バックエンドにNestJS、ORMにPrisma、データベースにPostgreSQL 15を使用しています。 工夫点として、PrismaなどのORMは、JSONへの全文検索インデックス作成を標準ではサポートしていません。そのため、マイグレーション管理とクエリ実行には Raw SQL（生SQL）を用いて実装を行いました。 -->

---

## 8. 検証データセット

検索精度の検証用として、LLM を用いて現実的なシードデータを生成。

- データ規模: 5000 件のアイテムデータ

- データ特性:
  - ポエム的な説明文

  - MD 由来の構造を持つ JSON 構造

これらを投入し、**キーワード検索のヒット**を検証。

<!-- LLMを用いて生成した5,000件のシードデータを使用しました。 生成にはスライド（※口頭では触れずともOK、想定プロンプトの内容）のようなプロンプトを使用し、「ユニークで想像力を掻き立てるタイトル」や「Markdown形式の説明文」を含む、リアリティのあるデータを生成させました。 単純なテストデータではなく、こうした複雑なデータセットを投入することで、ヒット動作の検証を行いました。 -->

---

## 9. デモンストレーション

実際にシステムを動かして検索を行います。

注目ポイント
レスポンス速度: シーケンシャルスキャンではない、インデックス検索の速さ

JSON 文脈検索: タイトルには含まれないが、「JSON 内の説明文」にのみキーワードが含まれるレコードがヒットすること

<!-- それでは実際にシステムを動かして検索を行います。 -->
